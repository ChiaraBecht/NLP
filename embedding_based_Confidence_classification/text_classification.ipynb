{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "from flair.embeddings import TokenEmbeddings\n",
    "from flair.data import Sentence\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading vocab.txt: 100%|██████████| 223k/223k [00:00<00:00, 620kB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sentence: \"The grass is green .\"]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init embedding\n",
    "#embedding = TransformerDocumentEmbeddings(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.43k/1.43k [00:00<00:00, 989kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 142kB/s]\n",
      "Downloading: 100%|██████████| 4.45k/4.45k [00:00<00:00, 3.09MB/s]\n",
      "Downloading: 100%|██████████| 691/691 [00:00<00:00, 500kB/s]\n",
      "Downloading: 100%|██████████| 124/124 [00:00<00:00, 92.1kB/s]\n",
      "Downloading: 100%|██████████| 767/767 [00:00<00:00, 535kB/s]\n",
      "Downloading: 100%|██████████| 433M/433M [00:06<00:00, 70.1MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 37.1kB/s]\n",
      "Downloading: 100%|██████████| 300/300 [00:00<00:00, 217kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 83.4kB/s]\n",
      "Downloading: 100%|██████████| 669k/669k [00:00<00:00, 1.43MB/s]\n",
      "Downloading: 100%|██████████| 412/412 [00:00<00:00, 308kB/s]\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 576kB/s] \n",
      "Downloading: 100%|██████████| 229/229 [00:00<00:00, 165kB/s]\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import SentenceTransformerDocumentEmbeddings\n",
    "# source: https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\n",
    "# init embedding\n",
    "embedding = SentenceTransformerDocumentEmbeddings(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://huggingface.co/gsarti/scibert-nli\n",
    "sci_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceEmbedding(torch.nn.Module):\n",
    "    def __init__(self, model, name):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.embedding_type = \"hugging_face_transformer\"\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        self.embedding_length = model.config.hidden_size\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        return outputs[0]\n",
    "    \n",
    "    def get_sentence_embedding(self, sentence):\n",
    "        input_ids = torch.tensor(self.tokenizer.encode(sentence, add_special_tokens=True)).unsqueeze(0) \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "        return outputs[0][0, 0, :]\n",
    "\n",
    "bio_model = AutoModel.from_pretrained(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "hf_embedding = HuggingFaceEmbedding(model = bio_model, name = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "bio_embedding = StackedEmbeddings([hf_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hugging face models and convert into flair models\n",
    "class HuggingFaceEmbedding(torch.nn.Module):\n",
    "    def __init__(self, model, name):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.embedding_type = \"hugging_face_transformer\"\n",
    "        self.model.eval()\n",
    "        self.embedding_length = model.config.hidden_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        return outputs[0]\n",
    "    \n",
    "    def flair_embedding(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        return outputs[2][:, 0, :]\n",
    "\n",
    "bio_model = AutoModel.from_pretrained(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "hf_embedding = HuggingFaceEmbedding(model = bio_model, name = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "bio_embedding = StackedEmbeddings([hf_embedding])\n",
    "#sci_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "#hf1_embedding = HuggingFaceEmbedding(model = sci_model, name = \"SciBertEmbedding\")\n",
    "#sci_embedding = StackedEmbeddings([hf1_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two sentence embeddings\n",
    "combined = [bio_embedding, sci_embedding]\n",
    "stack = StackedEmbeddings(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-04 19:10:37,680 Reading data from /students/2022-2023/master/spacey/chiara/Meta-knowledge_GENIA_corpus/corpus\n",
      "2023-02-04 19:10:37,681 Train: /students/2022-2023/master/spacey/chiara/Meta-knowledge_GENIA_corpus/corpus/train.csv\n",
      "2023-02-04 19:10:37,682 Dev: /students/2022-2023/master/spacey/chiara/Meta-knowledge_GENIA_corpus/corpus/dev.csv\n",
      "2023-02-04 19:10:37,682 Test: /students/2022-2023/master/spacey/chiara/Meta-knowledge_GENIA_corpus/corpus/test.csv\n",
      "Corpus: 2374 train + 607 dev + 607 test sentences\n"
     ]
    }
   ],
   "source": [
    "# Load tagged corpus\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = r'/students/2022-2023/master/spacey/chiara/Meta-knowledge_GENIA_corpus/corpus/'\n",
    "#print(data_folder)\n",
    "column_name_map = {0: \"text\", 1:\"label\"}  \n",
    "label_type = \"label\" \n",
    "\n",
    "corpus = CSVClassificationCorpus(data_folder, \n",
    "                                        column_name_map, \n",
    "                                        skip_header=True, \n",
    "                                        delimiter=\"|\",\n",
    "                                        train_file= 'train.csv',\n",
    "                                        dev_file= 'dev.csv',\n",
    "                                        test_file= 'test.csv', \n",
    "                                        label_type=label_type)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-04 19:10:41,504 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2374it [00:01, 2057.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-04 19:10:42,662 Dictionary created for label 'label' with 4 values: L2 (seen 1068 times), L3 (seen 1000 times), L1 (seen 306 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = corpus.make_label_dictionary(label_type=label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TextClassifier(bio_embedding, label_dictionary = label_dict, label_type = label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-04 19:24:03,708 ----------------------------------------------------------------------------------------------------\n",
      "2023-02-04 19:24:03,711 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=768, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (document_embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): HuggingFaceEmbedding(\n",
      "      (model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2023-02-04 19:24:03,712 ----------------------------------------------------------------------------------------------------\n",
      "2023-02-04 19:24:03,713 Corpus: \"Corpus: 2374 train + 607 dev + 607 test sentences\"\n",
      "2023-02-04 19:24:03,714 ----------------------------------------------------------------------------------------------------\n",
      "2023-02-04 19:24:03,715 Parameters:\n",
      "2023-02-04 19:24:03,715  - learning_rate: \"0.100000\"\n",
      "2023-02-04 19:24:03,716  - mini_batch_size: \"32\"\n",
      "2023-02-04 19:24:03,716  - patience: \"3\"\n",
      "2023-02-04 19:24:03,717  - anneal_factor: \"0.5\"\n",
      "2023-02-04 19:24:03,718  - max_epochs: \"4\"\n",
      "2023-02-04 19:24:03,718  - shuffle: \"True\"\n",
      "2023-02-04 19:24:03,719  - train_with_dev: \"False\"\n",
      "2023-02-04 19:24:03,719  - batch_growth_annealing: \"False\"\n",
      "2023-02-04 19:24:03,720 ----------------------------------------------------------------------------------------------------\n",
      "2023-02-04 19:24:03,721 Model training base path: \"classifier\"\n",
      "2023-02-04 19:24:03,721 ----------------------------------------------------------------------------------------------------\n",
      "2023-02-04 19:24:03,722 Device: cpu\n",
      "2023-02-04 19:24:03,723 ----------------------------------------------------------------------------------------------------\n",
      "2023-02-04 19:24:03,723 Embeddings storage mode: cpu\n",
      "2023-02-04 19:24:03,724 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceEmbedding' object has no attribute 'embed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [68], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(classifier, corpus)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/commons/conda/spacy/lib/python3.10/site-packages/flair/trainers/trainer.py:500\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self, base_path, learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, train_with_dev, train_with_test, monitor_train, monitor_test, main_evaluation_metric, scheduler, anneal_factor, patience, min_learning_rate, initial_extra_patience, optimizer, cycle_momentum, warmup_fraction, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, anneal_against_dev_loss, batch_growth_annealing, shuffle, param_selection_mode, write_weights, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, save_model_each_k_epochs, tensorboard_comment, use_swa, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, create_file_logs, create_loss_file, epoch, use_tensorboard, tensorboard_log_dir, metrics_for_tensorboard, optimizer_state_dict, scheduler_state_dict, save_optimizer_state, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39m# forward and backward for batch\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39mfor\u001b[39;00m batch_step \u001b[39min\u001b[39;00m batch_steps:\n\u001b[1;32m    498\u001b[0m \n\u001b[1;32m    499\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward_loss(batch_step)\n\u001b[1;32m    502\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loss, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    503\u001b[0m         average_over \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/commons/conda/spacy/lib/python3.10/site-packages/flair/nn/model.py:588\u001b[0m, in \u001b[0;36mDefaultClassifier.forward_loss\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_loss\u001b[39m(\u001b[39mself\u001b[39m, sentences: Union[List[DT], DT]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    586\u001b[0m \n\u001b[1;32m    587\u001b[0m     \u001b[39m# make a forward pass to produce embedded data points and labels\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m     embedded_data_points, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_pass(sentences)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    590\u001b[0m     \u001b[39m# no loss can be calculated if there are no labels\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(labels):\n",
      "File \u001b[0;32m/commons/conda/spacy/lib/python3.10/site-packages/flair/models/text_classification_model.py:62\u001b[0m, in \u001b[0;36mTextClassifier.forward_pass\u001b[0;34m(self, sentences, for_prediction)\u001b[0m\n\u001b[1;32m     59\u001b[0m     sentences \u001b[39m=\u001b[39m [sentences]\n\u001b[1;32m     61\u001b[0m \u001b[39m# embed sentences\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_embeddings\u001b[39m.\u001b[39;49membed(sentences)\n\u001b[1;32m     64\u001b[0m \u001b[39m# make tensor for all embedded sentences in batch\u001b[39;00m\n\u001b[1;32m     65\u001b[0m embedding_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_embeddings\u001b[39m.\u001b[39mget_names()\n",
      "File \u001b[0;32m/commons/conda/spacy/lib/python3.10/site-packages/flair/embeddings/token.py:68\u001b[0m, in \u001b[0;36mStackedEmbeddings.embed\u001b[0;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[1;32m     65\u001b[0m     sentences \u001b[39m=\u001b[39m [sentences]\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m embedding \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings:\n\u001b[0;32m---> 68\u001b[0m     embedding\u001b[39m.\u001b[39;49membed(sentences)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceEmbedding' object has no attribute 'embed'"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "trainer.train('classifier', learning_rate=0.1, mini_batch_size = 32, max_epochs = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('spacy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "049351683cbfcc6592fe172be1f7bdb3ebd27b203ddaf2bcbd9755e634118e44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
